{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Домашнее задание (50 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "В этом домашнем задании вы познакомитесь с основами NLP, научитесь обрабатывать тексты.\n",
    "\n",
    "В местах, где используется `...` (elipsis), требуется заменить его на код.\n",
    "\n",
    "Установим необходимые зависимости:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T19:21:30.570036Z",
     "start_time": "2024-11-27T19:21:27.452197Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install -U pip\n",
    "# !pip install nltk tqdm seqeval scikit-learn datasets numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T19:21:40.162946Z",
     "start_time": "2024-11-27T19:21:40.157819Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Professional\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Professional\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Professional\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict, Tuple, Callable\n",
    "from collections import Counter, defaultdict\n",
    "import unicodedata\n",
    "import random\n",
    "import math\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from datasets import load_dataset\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"wordnet\")\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## Токенизация (15 баллов)\n",
    "\n",
    "Токенизация - это процесс преобразования текста в набор токенов.\n",
    "Наивная реализация разбивает текст по пробелам. Более умные реализации учитывают пунктуацию.\n",
    "\n",
    "### Библиотека NLTK (2 балла)\n",
    "\n",
    "Научимся работать с токенизацией NLTK, где уже реализована работа с пунктуацией.\n",
    "\n",
    "https://www.nltk.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_tokenize OK\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text: str, language: str = \"english\") -> List[str]:\n",
    "    # используйте функцию nltk.word_tokenize для разбиения текста на токены\n",
    "    # https://www.nltk.org/api/nltk.tokenize.word_tokenize.html\n",
    "    return nltk.tokenize.word_tokenize(text)\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "\n",
    "assert tokenize(\"\") == []\n",
    "assert tokenize(\"Hello, world!\") == [\"Hello\", \",\", \"world\", \"!\"]\n",
    "assert tokenize(\"EU rejects German call to boycott British lamb.\") == [\"EU\", \"rejects\", \"German\", \"call\", \"to\", \"boycott\", \"British\", \"lamb\", \".\"]\n",
    "#print(tokenize(\"Hello, world!\"))\n",
    "#print(tokenize(\"EU rejects German call to boycott British lamb.\"))\n",
    "print(\"test_tokenize OK\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нормализация (3 балла)\n",
    "\n",
    "Добавим нормализацию после токенизации. Пробуем [лемматизацию](https://www.nltk.org/api/nltk.stem.wordnet.html#nltk.stem.wordnet.WordNetLemmatizer) , [стемминг](https://www.nltk.org/api/nltk.stem.snowball.html#nltk.stem.snowball.EnglishStemmer) и [юникод](https://docs.python.org/3/library/unicodedata.html#unicodedata.normalize) нормализацию. Напишем функцию, которая будет принимать на вход токен после токенизации, нормализовать в NFC юникод форму, переводит в нижний регистр, лемматизирует слово и, если слово не изменилось после лемматизации, применяет стемминг.\n",
    "\n",
    "\n",
    "Создайте функцию `normalize`:\n",
    "   - Функция `normalize` должна принимать строку `token` и возвращать нормализованный токен.\n",
    "   - Примените к токену Unicode нормализацию с помощью `unicode_nfc_normalizer`.\n",
    "   - Преобразуйте токен в нижний регистр.\n",
    "   - Примените лемматизацию с помощью `lemmatizer`.\n",
    "   - Если лемматизированный токен отличается от исходного, верните его. В противном случае, примените стемминг с помощью `stemmer` и верните результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_normalize OK\n"
     ]
    }
   ],
   "source": [
    "stemmer = EnglishStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "unicode_nfc_normalizer = lambda token: unicodedata.normalize(\"NFC\", token)\n",
    "\n",
    "def normalize(token: str) -> str:\n",
    "    \"\"\"\n",
    "    Нормализует токен, применяя Unicode нормализацию, преобразование в нижний регистр,\n",
    "    лемматизацию и стемминг при необходимости.\n",
    "\n",
    "    :param token: Токен для нормализации\n",
    "    :return: Нормализованный токен\n",
    "    \"\"\"\n",
    "    normalized = unicode_nfc_normalizer(token) #применяем Unicode nfc нормализацию\n",
    "    lower = normalized.lower() #переводим в lower_case\n",
    "    lemmatized = lemmatizer.lemmatize(lower) #лемматизируем\n",
    "    \n",
    "    if lemmatized != lower: return lemmatized #если лемматизированный токен отличается то возвращаем егоо\n",
    "    else: return stemmer.stem(lemmatized) #иначе применяем стемминг\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "test_tokens = [\"Worlds\", \"churches\", \"Helping\"]\n",
    "assert [normalize(token) for token in test_tokens] == [\"world\", \"church\", \"help\"]\n",
    "print(\"test_normalize OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Добавляем Словарь (10 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Современные токенайзеры не только разбивают строки на токены, но и преобразуют последовательность токенов в последовательность числел. Объединим функцию токенизации, нормализации и отображения из токенов в индексы в один объект токенайзера.\n",
    "\n",
    "Напишите класс `Tokenizer` для токенизации и нормализации текста.\n",
    "\n",
    "Построение словаря:\n",
    "   - Создайте метод `_build_vocabulary`, который принимает список текстов `texts` и обновляет словарь токенов.\n",
    "   - Для каждого текста:\n",
    "     - Токенизируйте и нормализуйте текст.\n",
    "     - Обновите счетчик вхождений слов.\n",
    "   - После обработки всех текстов для каждого слова, которое встречается не менее `min_count` раз, добавьте слово в словарь `word2idx` и список `idx2word`.\n",
    "\n",
    "Кодирование и декодирование:\n",
    "   - Создайте метод `encode_word`, который принимает слово `word` и возвращает его индекс с применением нормализации.\n",
    "   - Создайте метод `encode`, который принимает текст `text` и возвращает список индексов токенов.\n",
    "   - Создайте метод `decode`, который принимает список индексов `input_ids` и возвращает текст, вставляя пробелы между токенами.\n",
    "\n",
    "> Note: для функций, которые могут долго исполнятся (`_build_vocab`), рекомендуется использовать библиотеку tqdm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_tokenizer OK\n"
     ]
    }
   ],
   "source": [
    "class Tokenizer:\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            texts: List[str], \n",
    "            min_count: int = 1,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Инициализация токенизатора.\n",
    "\n",
    "        :param texts: список текстов для построения словаря\n",
    "        :param tokenize_fn: функция для токенизации текста\n",
    "        :param normalize_fn: функция для нормализации токенов\n",
    "        :param min_count: минимальное количество вхождений слова для включения в словарь\n",
    "        \"\"\"\n",
    "        self.min_count = min_count\n",
    "        self.word2idx = {\"<PAD>\": 0, \"<BOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
    "        self.unk_token_id = 3\n",
    "        self.idx2word = [\"<PAD>\", \"<BOS>\", \"<EOS>\", \"<UNK>\"]\n",
    "        self.word2count = Counter()\n",
    "        self._build_vocabulary(texts)\n",
    "\n",
    "    def _build_vocabulary(self, texts: List[str]):\n",
    "        \"\"\"\n",
    "        Построение словаря на основе списка текстов.\n",
    "\n",
    "        :param texts: список текстов\n",
    "        \"\"\"        \n",
    "        for text in texts:\n",
    "            tokens = tokenize(text)\n",
    "            normalized_tokens = [normalize(token) for token in tokens]                          \n",
    "            self.word2count.update(normalized_tokens)\n",
    "            \n",
    "        # Теперь у нас есть заполненный self.word2count\n",
    "        # ключи - нормализованные токены, значения - их встерчаемость в тексте\n",
    "        # нужно добавить в словарь новый токен (обновить self.word2idx и self.idx2word), \n",
    "        # если его встречаемость не меньше self.min_count\n",
    "        for word, count in self.word2count.items():\n",
    "            if count >= self.min_count and word not in self.word2idx:\n",
    "                self.word2idx[word] = len(self.word2idx)\n",
    "                self.idx2word.append(word)\n",
    "        \n",
    "\n",
    "    def encode_word(self, text: str) -> int:\n",
    "        \"\"\"\n",
    "        Кодирование слова в индекс с применением нормализации.\n",
    "\n",
    "        :param text: слово\n",
    "        :return: индекс слова\n",
    "        \"\"\"\n",
    "        if normalize(text) not in self.word2idx: \n",
    "            return self.unk_token_id\n",
    "        else: \n",
    "            return self.word2idx.get(normalize(text))\n",
    "        \n",
    "        raise NotImplementedError()\n",
    "    \n",
    "\n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        \"\"\"\n",
    "        Кодирование текста в набор индексов.\n",
    "\n",
    "        :param text: текст\n",
    "        :return: набор индексов токенов\n",
    "        \"\"\"\n",
    "        tokens = tokenize(text)\n",
    "        out_ids = []\n",
    "        for token in tokens:\n",
    "            out_ids.append(self.encode_word(token))\n",
    "        \n",
    "        return out_ids\n",
    "    \n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def decode(self, input_ids: List[int]) -> str:\n",
    "        \"\"\"\n",
    "        Декодирование набора индексов в текст. Вставляет пробел между декодированнми токенами.\n",
    "\n",
    "        :param input_ids: набор индексов токенов\n",
    "        :return: текст\n",
    "        \"\"\"\n",
    "        encoded = \" \".join(self.idx2word[ind] for ind in input_ids)\n",
    "        return encoded\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Возвращает количество уникальных токенов в словаре.\n",
    "\n",
    "        :return: количество уникальных токенов\n",
    "        \"\"\"\n",
    "        return len(self.word2idx)\n",
    "\n",
    "    def __contains__(self, item: str) -> bool:\n",
    "        \"\"\"\n",
    "        Проверяет, содержится ли слово в словаре.\n",
    "\n",
    "        :param item: слово\n",
    "        :return: True, если слово содержится в словаре, иначе False\n",
    "        \"\"\"\n",
    "        return item in self.word2idx\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Возвращает строковое представление словаря.\n",
    "\n",
    "        :return: строковое представление словаря\n",
    "        \"\"\"\n",
    "        return str(self.word2idx)\n",
    "\n",
    "\n",
    "corpus = [\"Hello, world!\", \"I love Python!\"]\n",
    "tokenizer = Tokenizer(corpus, min_count=1)\n",
    "encoded = tokenizer.encode(\"Hello, Python! I love you\")\n",
    "assert tokenizer.decode(encoded) == \"hello , python ! i love <UNK>\"\n",
    "print(\"test_tokenizer OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF (20 баллов)\n",
    "\n",
    "\n",
    "### Класс TFIDF (10 баллов)\n",
    "\n",
    "Создайте класс `TFIDF` для вычисления TF-IDF значений.\n",
    "\n",
    "Вам нужно реализовать следующие функции:\n",
    "### add_doc\n",
    "0. Увеличиваем счетчик num_docs - число документов в обучающей выборке\n",
    "1. Токенизируем текст\n",
    "2. Берем уникальные токены\n",
    "3. Обновляем self.term2num_docs - массив, в котором для каждого токена хранится число того, в скольких уникальных документах этот токен встречается. Токен `<UNK>` игнорируем.\n",
    "\n",
    "### idf\n",
    "Считаем логарифмированный inverse document frequency для документа\n",
    "\n",
    "$$\n",
    "idf = -log \\frac {n_t + 1} {N}\n",
    "$$\n",
    "где $n_t$ - в сколькиг документах встречается токен, $N$ - число различных документов. За эти параметры у нас отвечают параметры `self.term2num_docs` и `self.num_docs`. (единицы мы добавляем, чтобы избежа\n",
    "\n",
    "### predict\n",
    "1. Получаем набор документов, на выходе генерируем numpy матрицу tf-idf размера len(docs) x len(vocabulary)\n",
    "2. Токенизируем каждый текст\n",
    "3. Для каждого уникального токена в тексте (кроме `<UNK>`) считаем tf - как часто данный токен встречается во всем тексте (с учетом дублей и `<UNK>` токена!)\n",
    "4. Для каждого токена считаем idf из одноименной функции\n",
    "5. Заполняем соответствующие элементы массива\n",
    "6. Нормализуем вектор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDF:\n",
    "    def __init__(self, tokenizer: Tokenizer) -> None:\n",
    "        \"\"\"\n",
    "        Инициализация TFIDF.\n",
    "\n",
    "        :param tokenizer: токенизатор для преобразования текста в токены\n",
    "        :param default_idf: значение IDF для неизвестных токенов\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.num_docs = 0\n",
    "        self.term2num_docs = [0 for _ in tokenizer.word2idx]  # для подсчёта IDF\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        \"\"\"\n",
    "        Возвращает размер словаря.\n",
    "\n",
    "        :return: размер словаря\n",
    "        \"\"\"\n",
    "        return len(self.tokenizer)\n",
    "    \n",
    "    def add_doc(self, doc: str) -> None:\n",
    "        \"\"\"\n",
    "        Добавляет документ в модель TFIDF.\n",
    "        1. Увеличиваем счетчик числа документов\n",
    "        2. Токенизируем текст\n",
    "        3. Для всех уникальных токенов обновляем self.term2num_docs для подсчета IDF\n",
    "\n",
    "        :param doc: документ для добавления\n",
    "        \"\"\"\n",
    "        self.num_docs += 1\n",
    "        tokens = tokenize(doc)\n",
    "        unique_tokens = set(normalize(token) for token in tokens)\n",
    "        \n",
    "        for token in unique_tokens:\n",
    "            if token in self.tokenizer.word2idx and self.tokenizer.word2idx[token] != self.tokenizer.unk_token_id:\n",
    "                token_id = self.tokenizer.word2idx[token]\n",
    "                self.term2num_docs[token_id] += 1  # Увеличиваем счетчик документов для этого токена\n",
    "\n",
    "    def fit(self, docs: List[str]) -> None:\n",
    "        \"\"\"\n",
    "        Обучает модель TFIDF на корпусе docs.\n",
    "\n",
    "        :param docs: корпус для обучения\n",
    "        \"\"\"\n",
    "        for doc in docs:\n",
    "            self.add_doc(doc)\n",
    "\n",
    "    def predict(self, docs: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Предсказывает TFIDF значения для списка документов.\n",
    "        1. Создаем np.array размерности (len(docs), vocab_size)\n",
    "        2. Для каждого документа\n",
    "            а. Токенизируем его\n",
    "            б. Считаем tf для каждого токена кроме unk\n",
    "            в. Считаем idf для каждого токена кроме unk\n",
    "            г. Заполняем соответствующее значение в матрице\n",
    "        3. Нормализуем матрицу по размерности словаря (по строкам). При нормализации, \n",
    "        чтобы избежать деления на 0 делите не на норму ветктора, а на норму вектора + 1e-5\n",
    "\n",
    "        :param docs: список документов\n",
    "        :return: матрица TFIDF значений\n",
    "        \"\"\"\n",
    "        # создаем numpy массив нулей размера len(docs) на размерность словаря\n",
    "        result = np.zeros((len(docs), self.vocab_size))\n",
    "        # для каждого документа будем считать его вектор tf-idf\n",
    "        for doc_idx, document_text in enumerate(docs):\n",
    "            tokens = tokenize(document_text)\n",
    "            normalized_tokens = [normalize(token) for token in tokens]\n",
    "            token_counts = Counter(normalized_tokens) \n",
    "\n",
    "            for token, count in token_counts.items():\n",
    "                if token in self.tokenizer.word2idx and self.tokenizer.word2idx[token] != self.tokenizer.unk_token_id:\n",
    "                    token_id = self.tokenizer.word2idx[token]\n",
    "                    tf = count / len(normalized_tokens)\n",
    "                    idf = self.idf(token_id) \n",
    "                    result[doc_idx, token_id] = tf * idf\n",
    "        norm = np.linalg.norm(result, axis=1, keepdims=True)\n",
    "        # нормализуем документ\n",
    "        result = result / (norm + 1e-5)\n",
    "        return result\n",
    "\n",
    "    def idf(self, token_id: int) -> float:\n",
    "        \"\"\"\n",
    "        Вычисляет IDF (обратную частоту документа) для термина.\n",
    "\n",
    "        :param token_id: ID токена\n",
    "        :return: IDF значение\n",
    "        \"\"\"\n",
    "        n_t = self.term2num_docs[token_id]\n",
    "        N = self.num_docs\n",
    "        if N == 0:  \n",
    "            return 0.0 \n",
    "\n",
    "        return -math.log((n_t + 1) / N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_tfidf OK\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    \"hello there\", \"my favourite frut is\", \"i love bananas\", \"hello mama\", \"I need to eat\",\n",
    "    \"how can I get to the studio\", \"bottom gear\"\n",
    "]\n",
    "tokenizer = Tokenizer(corpus, min_count=1)\n",
    "tfidf = TFIDF(tokenizer)\n",
    "tfidf.fit(corpus)\n",
    "\n",
    "assert not np.any(tfidf.predict([\"all tokens abscent from vocab should be zeros vector\"]))\n",
    "reference = np.zeros((1, len(tfidf.tokenizer)))\n",
    "reference[0, tfidf.tokenizer.encode_word(\"hello\")] = 2 / 3 * -np.log(3/7)\n",
    "reference[0, tfidf.tokenizer.encode_word(\"mama\")] = 1 / 3 * -np.log(2/7)\n",
    "reference /= 0.7024715222440031\n",
    "assert np.allclose(tfidf.predict([\"hello hello mama\"]), reference)\n",
    "print(\"test_tfidf OK\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классификация с помощью TF-IDF - 10 баллов\n",
    "В этом задании предлагается обучить с помощью полученного векторизатора TF-IDF логистическую регресиию.\n",
    "Для этого мы возьмем корпус IMDB - отзывы фильмов. Это задача бинарной классификации, в которой нужно определить - положительный отзыв или отрицательный.\n",
    "\n",
    "Задача будет решаться следующими этапами:\n",
    "1. Загружаем текстовый корпус, обучаем словарь и TFIDF\n",
    "2. Векторизуем корпус текстов\n",
    "3. По векторизованному корпусу и меткам текстов обучаем логистическую регрессию, смотрим на качество на тесте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загружаем датасет\n",
    "imdb = load_dataset(\"stanfordnlp/imdb\")\n",
    "train_dataset = [imdb[\"train\"][i] for i in range(10_000, 15_000)]\n",
    "test_dataset = [imdb[\"test\"][i] for i in range(10_000, 15_000)]\n",
    "\n",
    "print(\"Label\")\n",
    "print(train_dataset[0][\"label\"])\n",
    "print(\"Text\")\n",
    "print(train_dataset[0][\"text\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = [sample[\"text\"] for sample in train_dataset]\n",
    "test_texts = [sample[\"text\"] for sample in test_dataset]\n",
    "\n",
    "# Создаем токенизатор, min_count можете взять на свое усмотрение, но лучше брать в районе 5-10;\n",
    "\n",
    "# 4. Обучаем по ним LogisticRegression, accuracy на тесте должен быть 0.75+\n",
    "\n",
    "\n",
    "# 1. Токенизатор обучаем по входным текстам\n",
    "tokenizer = ...\n",
    "# 2. Обучаем по этим же текстам с этим токенайзером TFIDF\n",
    "tfidf = ...\n",
    "\n",
    "# 3. превращаем тексты в numpy матрицы\n",
    "X_train = ...\n",
    "X_test = ...\n",
    "\n",
    "Y_train = ...\n",
    "Y_test = ...\n",
    "\n",
    "# Обучаем логистическую регрессию и смотрим на качество\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, Y_train)\n",
    "prediction = clf.predict(X_test)\n",
    "\n",
    "accuracy = (prediction == Y_test).sum() / Y_test.shape[0]\n",
    "print(f\"accuracy = {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь обучим логистическую регрессию со второй TF-IDF моделью и сравним результаты. Для этого воспользуйтесь классом [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) из sklearn. Результаты должны быть похожи на вашу реализацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = [sample[\"text\"] for sample in train_dataset]\n",
    "test_texts = [sample[\"text\"] for sample in test_dataset]\n",
    "tfidf = ...\n",
    "\n",
    "X_train = ...\n",
    "X_test = ...\n",
    "\n",
    "Y_train = np.array([sample[\"label\"] for sample in train_dataset])\n",
    "Y_test = np.array([sample[\"label\"] for sample in test_dataset])\n",
    "\n",
    "# Обучаем логистическую регрессию и смотрим на качество\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, Y_train)\n",
    "prediction = clf.predict(X_test)\n",
    "\n",
    "accuracy = (prediction == Y_test).sum() / Y_test.shape[0]\n",
    "print(f\"accuracy = {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $n$-граммные языковые модели (15 баллов)\n",
    "\n",
    "### Расширяем Токенайзер (3 балла)\n",
    "\n",
    "Перед созданием языковой модели, расширим токенизационный класс. Добавим два флага в сигнатуру метода `encode`, чтобы управлять добавлением служебных токенов во время токенизации. Существующий метод `decode` уже пропускает `<PAD>` токен, добавим флаг `skip_special_tokens` для пропуска всех специальных токенов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoSTokenizerEoS(Tokenizer):\n",
    "    def encode(self, text: str, add_bos: bool = True, add_eos: bool = False) -> List[int]:\n",
    "        \"\"\"\n",
    "        Кодирование текста в набор индексов.\n",
    "\n",
    "        :param text: текст\n",
    "        :param add_bos: добавление begin-of-sentence токена в начало\n",
    "        :param add_eos: добавление end-of-sentence токена в конец\n",
    "        :return: набор индексов токенов\n",
    "        \"\"\"\n",
    "        raise NotImplemented()\n",
    "\n",
    "    def decode(self, input_ids: List[int], skip_special_tokens: bool = True) -> str:\n",
    "        \"\"\"\n",
    "        Декодирование набора индексов в текст. Вставляет пробел между декодированнми токенами.\n",
    "\n",
    "        :param input_ids: набор индексов токенов\n",
    "        :param skip_special_tokens: пропуск специальных токенов во время декодирования.\n",
    "        :return: текст\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corpus = [\"Hello, world!\", \"I love Python!\", \"Hello, Python\", \"Hello there <EOS>\"]\n",
    "tokenizer = BoSTokenizerEoS(corpus, min_count=1)\n",
    "assert tokenizer.encode(\"hello world\", add_bos=True, add_eos=True) == [1, 4, 8, 2]\n",
    "assert tokenizer.encode(\"hello world\", add_bos=False, add_eos=False) == [4, 8]\n",
    "print(\"test_bos_tokenizer OK\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создаём NGram Модель (12 баллов)\n",
    "\n",
    "Создайте класс `NGramLanguageModel` для построения n-граммной языковой модели. В этом задании вы можете как опираться на предложенную структуру модели, так и сделать свою имплементацию.\n",
    "\n",
    "Построение модели:\n",
    "   - Создайте метод `_build_model`, который принимает список текстов `texts` и обновляет частоты n-грамм.\n",
    "   - Для каждого текста:\n",
    "     - Токенизируйте текст и добавьте токен `\"<EOS>\"` в конец.\n",
    "     - Для каждого токена:\n",
    "       - Определите префикс длиной `n-1`.\n",
    "       - Обновите частоты n-грамм и частоты префиксов.\n",
    "\n",
    "Генерация следующего токена:\n",
    "   - Создайте метод `generate_next_token`, который принимает префикс `prefix` и возвращает следующий токен.\n",
    "   - Преобразуйте префикс в кортеж.\n",
    "   - Получите распределение частот для префикса.\n",
    "   - Если распределение пустое, верните токен `\"<UNK>\"`.\n",
    "   - Верните токен с наибольшей частотой.\n",
    "   - Если включен флаг sample возьмите токен пропорционально встречаемости\n",
    "\n",
    "Автодополнение текста:\n",
    "   - Создайте метод `autocomplete`, который принимает текст `text` и максимальную длину `max_len`, и возвращает завершенный текст.\n",
    "   - Токенизируйте текст.\n",
    "   - Пока длина токенов меньше `max_len`:\n",
    "     - Определите префикс длиной `n-1`.\n",
    "     - Сгенерируйте следующий токен.\n",
    "     - Добавьте токен в список токенов.\n",
    "     - Если токен равен `\"<EOS>\"`, завершите генерацию.\n",
    "   - Декодируйте и верните текст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class NGramLanguageModel:\n",
    "    def __init__(self, n: int, tokenizer: BoSTokenizerEoS, texts: List[str]):\n",
    "        \"\"\"\n",
    "        Создание n-граммной языковой модели.\n",
    "\n",
    "        :param n: порядок n-грамм\n",
    "        :param vocabulary: словарь\n",
    "        \"\"\"\n",
    "        assert n >= 2\n",
    "        self.n = n\n",
    "        self.tokenizer = tokenizer\n",
    "        # Словарь, ключ которого - префикс\n",
    "        # значение - Counter(), в котором ключ это продолжение n-граммы, \n",
    "        # а значение - частота встречи этого продолжения\n",
    "        self.frequencies = defaultdict(lambda: Counter())  # частота n-грамм\n",
    "        self._build_model(texts)\n",
    "\n",
    "    def _build_model(self, texts: List[str]):\n",
    "        \"\"\"\n",
    "        Построение модели на основе списка текстов.\n",
    "\n",
    "        :param texts: список текстов\n",
    "        \"\"\"\n",
    "        raise NotImplemented()\n",
    "\n",
    "\n",
    "    def generate_next_token(self, prefix: List[int], sample: bool = False) -> int:\n",
    "        \"\"\"\n",
    "        Генерация следующего токена по префиксу.\n",
    "\n",
    "        :param prefix: префикс\n",
    "        :param sample: используем ли мы сэмплинг в генерации\n",
    "        :return: следующий токен\n",
    "        \"\"\"\n",
    "        raise NotImplemented()\n",
    "\n",
    "    def autocomplete(self, text: str, max_len: int = 32, \n",
    "                     skip_special_tokens: bool = True, sample: bool = False) -> str:\n",
    "        \"\"\"\n",
    "        Автоматическое дополнение текста.\n",
    "\n",
    "        :param text: текст\n",
    "        :param max_len: максимальная длина текста\n",
    "        :param skip_special_tokens: пропуск специальных токенов во время декодирования.\n",
    "        :return: завершенный текст\n",
    "        \"\"\"\n",
    "        raise NotImplemented()\n",
    "\n",
    "corpus = [\"Hello, world!\", \"I love Python!\", \"Hello, Python\"]\n",
    "tokenizer = BoSTokenizerEoS(corpus, min_count=1)\n",
    "ngram_lm = NGramLanguageModel(2, tokenizer, corpus)\n",
    "\n",
    "assert ngram_lm.autocomplete(\"Hello, Python\", max_len=10) == \"hello , python !\"\n",
    "assert ngram_lm.autocomplete(\"Hello, Python\", max_len=10, skip_special_tokens=False) == \"<BOS> hello , python ! <EOS>\"\n",
    "\n",
    "imdb = load_dataset(\"stanfordnlp/imdb\")\n",
    "train_texts = [imdb[\"train\"][i][\"text\"] for i in range(13_000, 15_000)]\n",
    "tokenizer = BoSTokenizerEoS(texts=train_texts, min_count=3)\n",
    "ngram_lm = NGramLanguageModel(3, tokenizer, train_texts)\n",
    "print(\"Продолжение фразы `the movie was`\")\n",
    "random.seed(1)\n",
    "print(ngram_lm.autocomplete(\"the movie was\", sample=True))\n",
    "print(\"test_ngram_model OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
